{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZD9VHj2xekO"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import os \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "import MySQLdb\n",
        "import numpy as np\n",
        "import base64\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load up the .env file\n",
        "try:\n",
        "    if (load_dotenv()):\n",
        "        print(\"Loaded .env variables\")\n",
        "    else:\n",
        "        print(\"Error in loading variables or file not found!\")\n",
        "\n",
        "except Exception:\n",
        "    print(\"Error in loading .env file\")\n",
        "## Connect to database\n",
        "host = os.environ.get(\"DB_HOST\")\n",
        "user = os.environ.get(\"DB_USER\")\n",
        "pw = os.environ.get(\"DB_PASS\")\n",
        "db_name = os.environ.get(\"DB_SCHEMA\")\n",
        "try:\n",
        "    connection = MySQLdb.connect(host, user, pw, db_name)\n",
        "    print(\"Successfully connected to \", db_name)\n",
        "    connection.close()\n",
        "except Exception:\n",
        "    print(\"Error connecting!\")\n",
        "    \n",
        "# Create an instance for the db and cursor\n",
        "def getDBCursor():\n",
        "    db = MySQLdb.connect(host, user, pw, db_name)\n",
        "    cursor = db.cursor()\n",
        "    return cursor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetching all captures from database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a global variable to store all IMG bytearray and data\n",
        "face_capture = []\n",
        "face_img = []\n",
        "\n",
        "# Face Entity and all Facts from the table \n",
        "face_id = []\n",
        "tenant_id = []\n",
        "tenant_name = []\n",
        "face_status = []\n",
        "\n",
        "\n",
        "def fetch_all_FaceCaptures():\n",
        "    '''\n",
        "        Begin making queries to the database and SELECT our FACE_IMG table\n",
        "        where all facial captures are stored\n",
        "    '''\n",
        "    # Create a query to select all facial captures\n",
        "    sql_fetch_faceCaptures = \"SELECT * FROM FACE_IMG\"\n",
        "    cursor = getDBCursor()\n",
        "    cursor.execute(sql_fetch_faceCaptures)\n",
        "    # Fetch all data as rows\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    # Iterate and assign data as to row\n",
        "    for row in rows:\n",
        "        face_id = row[0]\n",
        "        tenant_id = row[1]\n",
        "        tenant_name = row[2]\n",
        "        face_status = row[3]\n",
        "        face_capture = row[4]\n",
        "        # Convert the face_capture column to a bytes-like object\n",
        "        face_capture_bytes = bytearray(face_capture)\n",
        "        # Use the cv.imdecode() function to decode the bytes-like object into an image\n",
        "        face_img = cv.imdecode(np.frombuffer(face_capture_bytes, np.uint8), cv.IMREAD_COLOR)\n",
        "\n",
        "        # Display some attribute data \n",
        "        print(\"==============================================================\")\n",
        "        print(\"Face ID: \", face_id)\n",
        "        print(\"Tenant ID: \", tenant_id)\n",
        "        print(\"Tenant Full Name: \", tenant_name)\n",
        "        print(\"Status: \", face_status)\n",
        "        print(\"Face IMG data: \", face_img)\n",
        "        print(\" ---> Type: \", type(face_img))\n",
        "        print(\"==============================================================\")\n",
        "\n",
        "        # Render the image \n",
        "        try:\n",
        "            # Convert image to PNG format and store it in a variable\n",
        "            try: \n",
        "                # encoded_img = cv.imencode(\".png\", face_img)\n",
        "                # print(\"Converting image to .png\")\n",
        "                # face_img_png = encoded_img.tobytes()\n",
        "                # with open(f'out/{tenant_name}.png', 'wb') as f:\n",
        "                #     # Decode the base64-encoded PNG data\n",
        "                #     decoded_data = base64.b64decode(face_img_png)\n",
        "                #     print(f\"Saved PNG file: out/{tenant_name}.png\")\n",
        "                #     # Write the decoded data to the file stream\n",
        "                #     f.write(decoded_data)\n",
        "                if((os.path.exists('dataset/train/images'))):\n",
        "                        cv.imwrite(f'dataset/train/images/{tenant_name}.png', face_img)\n",
        "                        print(\"Images downloaded to\", f'dataset/train/images/{tenant_name}.png')\n",
        "                else: \n",
        "                    cv.imwrite(f'dataset/train/images/{tenant_name}.png', face_img)\n",
        "                    os.mkdir('dataset/train/images')\n",
        "                    print(\"Images downloaded to\", f'dataset/train/images/{tenant_name}.png')\n",
        "            except:\n",
        "                print(\"An error has occurred converting the image\")\n",
        "        except: \n",
        "            print(\"There was an error to rendering the image\")\n",
        "\n",
        "# Call the function to test it\n",
        "fetch_all_FaceCaptures()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Previewing our image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hpG2WSax4tN"
      },
      "outputs": [],
      "source": [
        "img = cv.imread(\"dataset/train/images/Cloyd Van Secuya.png\")\n",
        "# opencv BGR channel format and plt reads images as RGB channel format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "B6kZ0u1_yKM9",
        "outputId": "3d9301ce-9f77-40df-c477-748e4a73c566"
      },
      "outputs": [],
      "source": [
        "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "plt.imshow(img) # RGB "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Face Detection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using MTCNN we can initiate to detect faces from our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOZYOhPnyOPJ",
        "outputId": "61fd5174-ee48-4e16-e6b2-4d8af916388c"
      },
      "outputs": [],
      "source": [
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "detector = MTCNN()\n",
        "results = detector.detect_faces(img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Viewing an array of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfSiyHCJy1ai",
        "outputId": "805ed818-94f4-40b3-bec1-47bf766c66d4"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting the bounding areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8TFosN7y45_"
      },
      "outputs": [],
      "source": [
        "x,y,w,h = results[0]['box']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cropping the image based on bounding area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "N8ScrcjLzFXa",
        "outputId": "a4870ffc-4b05-4ff7-fdc9-cffe9abfcb94"
      },
      "outputs": [],
      "source": [
        "img = cv.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 30)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "CHaC2dNDzaAY",
        "outputId": "7a3e5835-6722-4d99-9aa0-0fc4b36532ac"
      },
      "outputs": [],
      "source": [
        "my_face = img[y:y+h, x:x+w]\n",
        "#Facenet takes as input 160x160 \n",
        "my_face = cv.resize(my_face, (160,160))\n",
        "plt.imshow(my_face)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Previewing the cropped image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfXqnuiZzmWQ",
        "outputId": "63e3d00f-0676-48f8-9c42-40cb7a8a4ee2"
      },
      "outputs": [],
      "source": [
        "my_face"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DFOJZSiN0LeL"
      },
      "source": [
        "### Creating a template for pre-processing \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we need to define a class to load, extract, and fetch our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U_uHILx0Bd8"
      },
      "outputs": [],
      "source": [
        "class FACELOADING:\n",
        "    ''' We need to read through the ../out directory '''\n",
        "    def __init__(self, directory):\n",
        "        print(\"Going into, \", directory)\n",
        "        self.directory = directory\n",
        "        self.target_size = (160,160)\n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "        self.detector = MTCNN()\n",
        "    \n",
        "\n",
        "    def extract_face(self, filename):\n",
        "        print(\"Reading images ====> \", filename)\n",
        "        img = cv.imread(filename)\n",
        "        print(\"Converting to RGB Channels\")\n",
        "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "        x,y,w,h = self.detector.detect_faces(img)[0]['box']\n",
        "        print(\"Bounding area: \")\n",
        "        print(\"\\t ===================\")\n",
        "        print(\"\\t x: \", x)\n",
        "        print(\"\\t y: \", y)\n",
        "        print(\"\\t width: \", w)\n",
        "        print(\"\\t height: \", h)\n",
        "        print(\"\\t ===================\")\n",
        "        x,y = abs(x), abs(y)\n",
        "        face = img[y:y+h, x:x+w]\n",
        "        print(\"Resizing: \", self.target_size)\n",
        "        face_arr = cv.resize(face, self.target_size)\n",
        "        return face_arr\n",
        "    \n",
        "\n",
        "    def load_faces(self, path):\n",
        "      FACES = []\n",
        "      print(\"Loading faces...\")\n",
        "      if not os.path.isfile(path):\n",
        "          return FACES\n",
        "      try:\n",
        "          single_face = self.extract_face(path)\n",
        "          FACES.append(single_face)\n",
        "      except Exception as e:\n",
        "          print(\"An error has occurred in loading facial capture images\")\n",
        "          pass\n",
        "      return FACES\n",
        "\n",
        "\n",
        "    def load_classes(self):\n",
        "        for sub_dir in os.listdir(self.directory):\n",
        "            path = os.path.join(self.directory, sub_dir)\n",
        "            print(\"Loading images from: \", path)\n",
        "            FACES = self.load_faces(path)\n",
        "            labels = [sub_dir for _ in range(len(FACES))]\n",
        "            self.X.extend(FACES)\n",
        "            self.Y.extend(labels)\n",
        "        return np.asarray(self.X), np.asarray(self.Y)\n",
        "\n",
        "    def plot_images(self):\n",
        "        print(\"Plotting images....\")\n",
        "        plt.figure(figsize=(18,16))\n",
        "        for num,image in enumerate(self.X):\n",
        "            ncols = 3\n",
        "            nrows = len(self.Y)//ncols + 1\n",
        "            plt.subplot(nrows,ncols,num+1)\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msAn6d322t4u",
        "outputId": "e28b0a91-4106-4e28-d8cb-7ea34ff7dce8"
      },
      "outputs": [],
      "source": [
        "faceloading = FACELOADING(\"dataset/train/images\")\n",
        "X, Y = faceloading.load_classes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "fh-AC5fL3khn",
        "outputId": "23533827-0a3c-4034-99ad-e4ba32c182d1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,12))\n",
        "for num,image in enumerate(X):\n",
        "    ncols = 3\n",
        "    nrows = len(Y)//ncols + 1\n",
        "    plt.subplot(nrows,ncols,num+1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WmV-atoG4LV-"
      },
      "source": [
        "### FaceNet Implementation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdUliUDp4b9y"
      },
      "outputs": [],
      "source": [
        "from keras_facenet import FaceNet\n",
        "embedder = FaceNet()\n",
        "\n",
        "def get_embedding(face_img):\n",
        "    face_img = face_img.astype('float32') # 3D(160x160x3)\n",
        "    face_img = np.expand_dims(face_img, axis=0) \n",
        "    # 4D (Nonex160x160x3)\n",
        "    yhat= embedder.embeddings(face_img)\n",
        "    return yhat[0] # 512D image (1x1x512)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rwgp-ML6Mxk",
        "outputId": "59b62d1e-a963-40ab-aa12-ebdc839757cc"
      },
      "outputs": [],
      "source": [
        "EMBEDDED_X = []\n",
        "\n",
        "for img in X:\n",
        "    EMBEDDED_X.append(get_embedding(img))\n",
        "\n",
        "EMBEDDED_X = np.asarray(EMBEDDED_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcitJ7DI-Ho9",
        "outputId": "2e412d93-c3d4-4b32-b467-6a0f39aaf947"
      },
      "outputs": [],
      "source": [
        "EMBEDDED_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7_ii2U6C-RZ",
        "outputId": "21c86153-c8b5-4c73-8317-8bdc3623a293"
      },
      "outputs": [],
      "source": [
        "EMBEDDED_X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGEZgJMA4PMJ"
      },
      "outputs": [],
      "source": [
        "np.savez_compressed('classes/faces_embeddings_done_4classes.npz', EMBEDDED_X, Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xAvTvFDC7FDO"
      },
      "source": [
        "### SVM Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZcM4_hZ7Y23"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "IC6pIaAy7BFo",
        "outputId": "5317cbf9-7c3b-4cd2-8bd0-39c8ee7ae119"
      },
      "outputs": [],
      "source": [
        "plt.plot(EMBEDDED_X[0]) \n",
        "plt.ylabel(Y[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XdzHJ6J7BCn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(EMBEDDED_X, Y, shuffle=True, random_state=17)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "azswDNGu70nL",
        "outputId": "b5df5a14-4a6c-4cfe-a24b-22b04d370da8"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyet_2cc8BZL"
      },
      "outputs": [],
      "source": [
        "ypreds_train = model.predict(X_train)\n",
        "ypreds_test = model.predict(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2JB8InT8JRs",
        "outputId": "b69c90aa-2592-475d-9f8d-ccda3e97aeb7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(Y_train, ypreds_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdACZovr8Q1z",
        "outputId": "896290a7-7854-4cb0-87f9-543634e957d8"
      },
      "outputs": [],
      "source": [
        "accuracy_score(Y_test,ypreds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7S-puaj8XWN",
        "outputId": "00a65476-8787-4db0-ff7e-c773daf58421"
      },
      "outputs": [],
      "source": [
        "t_im = cv.imread(\"dataset/test/images/Cloyd Van Secuya.png\")\n",
        "t_im = cv.cvtColor(t_im, cv.COLOR_BGR2RGB)\n",
        "x,y,w,h = detector.detect_faces(t_im)[0]['box']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d07QBDec85GG",
        "outputId": "d43ea795-ccff-4d67-f34c-5d82a9610760"
      },
      "outputs": [],
      "source": [
        "t_im = t_im[y:y+h, x:x+w]\n",
        "t_im = cv.resize(t_im, (160,160))\n",
        "test_im = get_embedding(t_im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsw3KKkd9HDf"
      },
      "outputs": [],
      "source": [
        "test_im = [test_im]\n",
        "ypreds = model.predict(test_im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfxTU8hA9KKP",
        "outputId": "4eba9fb6-abc4-4e0f-c854-773d242b868c"
      },
      "outputs": [],
      "source": [
        "ypreds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8vQxKv69kBt",
        "outputId": "8f9f4efb-a0d5-4ee6-ef57-5b1e58b3e6d0"
      },
      "outputs": [],
      "source": [
        "print(encoder.inverse_transform(ypreds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aTWqZCP9o51"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "#save the model\n",
        "with open('model/svm_model_160x160.pkl','wb') as f:\n",
        "    pickle.dump(model,f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
